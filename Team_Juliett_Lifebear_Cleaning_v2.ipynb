{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNpxcFzhuAY8"
      },
      "source": [
        "**Function to split chunks based on the chunksize**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHk3n7fvvrUU",
        "outputId": "be1a2933-33eb-48c0-f194-3f6dbb50dcc7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-bee1d5c59cbe>:19: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunksize, sep=sep)):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File '/content/lifebear.csv' split into 4 chunks in '/content/chunks'.\n"
          ]
        }
      ],
      "source": [
        "# prompt: Create a function to split a large csv into chunks in a specified folder or path using the chunksize parameter in read_csv\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "def split_csv_into_chunks(file_path, chunksize, output_directory, sep=','):\n",
        "  \"\"\"Splits a large CSV file into smaller chunks using the chunksize parameter.\n",
        "\n",
        "  Args:\n",
        "    file_path: The path to the large CSV file.\n",
        "    chunksize: The number of rows per chunk.\n",
        "    output_directory: The directory where the chunks should be saved.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if not os.path.exists(output_directory):\n",
        "      os.makedirs(output_directory)\n",
        "\n",
        "    for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunksize, sep=sep)):\n",
        "      output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "      chunk.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"File '{file_path}' split into {i+1} chunks in '{output_directory}'.\")\n",
        "\n",
        "  except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Replace 'your_large_file.csv' with the actual path to your file\n",
        "# Replace 'output_chunks_folder' with the desired output directory\n",
        "split_csv_into_chunks('/content/lifebear.csv', 1000000, '/content/chunks', sep=';')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck7XaHgC8jDI"
      },
      "source": [
        "Function to remove invalid rows from a CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvyt3sIN8pcW"
      },
      "outputs": [],
      "source": [
        "# prompt: Create a function that removes invalid rows from a CSV. Detect the number of columns based on the header and the specified delimiter. Rows where the number of columns don't match the expected number of columns and may cause problems when trying to use the read_csv function. Create a new CSV file with the valid rows.\n",
        "\n",
        "import csv\n",
        "\n",
        "def remove_invalid_rows(input_file, output_file, delimiter=','):\n",
        "  \"\"\"Removes invalid rows from a CSV file.\n",
        "\n",
        "  Args:\n",
        "    input_file: Path to the input CSV file.\n",
        "    output_file: Path to the output CSV file.\n",
        "    delimiter: Delimiter used in the CSV file.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
        "       open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
        "\n",
        "    reader = csv.reader(infile, delimiter=delimiter)\n",
        "    writer = csv.writer(outfile, delimiter=delimiter)\n",
        "\n",
        "    header = next(reader)  # Read the header row\n",
        "    expected_num_columns = len(header)\n",
        "    writer.writerow(header)  # Write the header to the output file\n",
        "\n",
        "    for row in reader:\n",
        "      if len(row) == expected_num_columns:\n",
        "        writer.writerow(row)\n",
        "\n",
        "# prompt: Create a function that removes invalid rows from a CSV. Detect the number of columns based on the header and the specified delimiter. Rows where the number of columns don't match the expected number of columns and may cause problems when trying to use the read_csv function.\n",
        "\n",
        "import csv\n",
        "\n",
        "def remove_invalid_rows_from_csv(csv_file_path, delimiter=','):\n",
        "  \"\"\"Removes rows from a CSV file that have an invalid number of columns.\n",
        "\n",
        "  Args:\n",
        "    csv_file_path: The path to the CSV file.\n",
        "    delimiter: The delimiter used in the CSV file.\n",
        "\n",
        "  Returns:\n",
        "    A list of valid rows.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(csv_file_path, 'r', encoding='utf-8') as file:\n",
        "    reader = csv.reader(file, delimiter=delimiter)\n",
        "    header = next(reader)  # Get the header row\n",
        "    expected_num_columns = len(header)\n",
        "    valid_rows = [header]  # Start with the header\n",
        "\n",
        "    for row in reader:\n",
        "      if len(row) == expected_num_columns:\n",
        "        valid_rows.append(row)\n",
        "      else:\n",
        "        print(f\"Warning: Skipping row with invalid number of columns: {row}\")\n",
        "\n",
        "  return valid_rows\n",
        "\n",
        "# Example usage:\n",
        "# valid_rows = remove_invalid_rows_from_csv('my_file.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMLEaIcl5PPY"
      },
      "source": [
        "Function to remove blank records or records with too many blank fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxRQ40PU5TxI"
      },
      "outputs": [],
      "source": [
        "# prompt: Create a function to remove blank records or records with too many blank fields either based on the number of columns or based on specific columns.Append the invalid records to a dataframe and drop them from the original dataframe. Return both the updated dataframe and the error dataframe. Include error checking and loggging.\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Function to remove blank records or records with too many blank fields.\n",
        "def remove_blank_records(df, threshold=None, specific_columns=None):\n",
        "    \"\"\"\n",
        "    Removes records with blank fields from a dataframe based on the threshold or specific columns.\n",
        "    Appends records with blank fields to a new dataframe and drops them from the original dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe to process.\n",
        "        threshold (int, optional): The minimum number of non-blank fields required in a record.\n",
        "            If None, removes records with all blank fields.\n",
        "        specific_columns (list, optional): A list of specific column names to check for blank fields.\n",
        "            If provided, threshold is ignored and records with any blank field in the specified columns\n",
        "            are removed.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the updated dataframe with valid records\n",
        "               and a new dataframe with records containing blank fields.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        error_df = pd.DataFrame()\n",
        "\n",
        "        if specific_columns:\n",
        "            for index, row in df.iterrows():\n",
        "                if any(pd.isnull(row[col]) or row[col] == '' for col in specific_columns):\n",
        "                    error_df = pd.concat([error_df, pd.DataFrame([row])], ignore_index=True)\n",
        "                    df.drop(index, inplace=True)\n",
        "        else:\n",
        "            for index, row in df.iterrows():\n",
        "                if threshold is None:\n",
        "                    if all(pd.isnull(val) or val == '' for val in row):\n",
        "                        error_df = pd.concat([error_df, pd.DataFrame([row])], ignore_index=True)\n",
        "                        df.drop(index, inplace=True)\n",
        "                else:\n",
        "                    non_blank_count = sum(1 for val in row if not (pd.isnull(val) or val == ''))\n",
        "                    if non_blank_count < threshold:\n",
        "                        error_df = pd.concat([error_df, pd.DataFrame([row])], ignore_index=True)\n",
        "                        df.drop(index, inplace=True)\n",
        "\n",
        "        print(\"Blank record removal complete. Records with blank fields appended to error_df.\")\n",
        "        return df, error_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during blank record removal: {e}\")\n",
        "        return df, pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKAuMsOY6bsD"
      },
      "source": [
        "Function to remove duplicate records based on specified columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FB4Uc1at68C_"
      },
      "outputs": [],
      "source": [
        "# prompt: Create a function to remove duplicate records based on specified columns. Add the duplicates records to separate dataframe and drop them from the original. Include error checking and logging.\n",
        "\n",
        "def remove_duplicate_records(df, columns):\n",
        "    \"\"\"\n",
        "    Removes duplicate records based on specified columns.\n",
        "    Adds duplicate records to a separate dataframe and drops them from the original.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe to process.\n",
        "        columns (list): A list of column names to consider for duplicate detection.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the updated dataframe with unique records\n",
        "               and a new dataframe with duplicate records.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        duplicate_df = pd.DataFrame()\n",
        "        df_deduplicated = df.drop_duplicates(subset=columns, keep='first')\n",
        "        duplicate_rows = df[~df.index.isin(df_deduplicated.index)]\n",
        "\n",
        "        if not duplicate_rows.empty:\n",
        "            duplicate_df = pd.concat([duplicate_df, duplicate_rows], ignore_index=True)\n",
        "\n",
        "        print(f\"Duplicate removal complete. Duplicate records appended to duplicate_df.\")\n",
        "        return df_deduplicated, duplicate_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during duplicate removal: {e}\")\n",
        "        return df, pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXISTWtz3xYE"
      },
      "source": [
        "Function to validate email addresses. It removes records with invalid email addresses and saves them to a separate dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Deih_Q3wb19V"
      },
      "outputs": [],
      "source": [
        "# prompt: create a function to validate emails addresses in a dataframe, append the records with an invalid email address to a dataframe and drop them from the original dataframe. Return both the updated dataframe and the error dataframe. Include error checking and loggging.\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def validate_and_remove_invalid_emails(df, email_column):\n",
        "    \"\"\"\n",
        "    Validates email addresses in a dataframe, appends records with invalid email\n",
        "    addresses to a new dataframe, and removes them from the original dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe containing email addresses.\n",
        "        email_column (str): The name of the column containing email addresses.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the updated dataframe with valid email addresses\n",
        "            and a new dataframe with records containing invalid email addresses.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Regular expression for basic email validation\n",
        "        email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
        "\n",
        "        # Create a new dataframe to store records with invalid email addresses\n",
        "        error_df = pd.DataFrame()\n",
        "\n",
        "        # Iterate through the dataframe and validate email addresses\n",
        "        for index, row in df.iterrows():\n",
        "            email = row[email_column]\n",
        "            if not re.match(email_regex, email):\n",
        "                # Append record to the error dataframe\n",
        "                error_df = pd.concat([error_df, pd.DataFrame([row])], ignore_index=True)\n",
        "                # Drop the record from the original dataframe\n",
        "                df.drop(index, inplace=True)\n",
        "\n",
        "        print(\"Validation complete. Invalid email records appended to error_df.\")\n",
        "        return df, error_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during email validation: {e}\")\n",
        "        return df, pd.DataFrame()  # Return empty error dataframe in case of error\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39aO6D5fccdV"
      },
      "source": [
        "Function to remove time from the date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48KrDgptcgUm"
      },
      "outputs": [],
      "source": [
        "# prompt: Create a function to remove the time from a date in specified columns\n",
        "\n",
        "def remove_time_from_date(df, columns):\n",
        "  \"\"\"Removes the time component from date columns in a DataFrame.\n",
        "\n",
        "  Args:\n",
        "    df: The DataFrame containing the date columns.\n",
        "    columns: A list of column names to process.\n",
        "\n",
        "  Returns:\n",
        "    The DataFrame with the time component removed from the specified columns.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    for column in columns:\n",
        "      if column in df.columns and pd.api.types.is_datetime64_any_dtype(df[column]):\n",
        "        df[column] = df[column].dt.date\n",
        "\n",
        "    return df\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Assuming your DataFrame is 'df' and you want to remove time from columns 'date_column1' and 'date_column2'\n",
        "# df = remove_time_from_date(df, ['date_column1', 'date_column2'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79ds6qQpAKL_"
      },
      "source": [
        "Function to validate phone numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVe2u4hmANxo"
      },
      "outputs": [],
      "source": [
        "# prompt: create a function to validate phone numbers in a dataframe including the characters \"+\", \"-\", \"(\", \")\", \".\"  If it is  valid phone number but includes these characters, remove them. Append the records with an invalid phone number to a dataframe and drop them from the original dataframe. Return both the updated dataframe and the error dataframe. Include error checking and logging.\n",
        "\n",
        "def validate_and_remove_invalid_phone_numbers(df, phone_column):\n",
        "    \"\"\"\n",
        "    Validates phone numbers in a dataframe, appends records with invalid phone\n",
        "    numbers to a new dataframe, and removes them from the original dataframe.\n",
        "    If a phone number is valid but contains special characters, they are removed.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe containing phone numbers.\n",
        "        phone_column (str): The name of the column containing phone numbers.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the updated dataframe with valid phone numbers\n",
        "            and a new dataframe with records containing invalid phone numbers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        error_df = pd.DataFrame()\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            phone_number = row[phone_column]\n",
        "\n",
        "            # Remove special characters from the phone number\n",
        "            phone_number = re.sub(r\"[+()\\-. ]\", \"\", str(phone_number))\n",
        "\n",
        "            # Check if the phone number contains only digits\n",
        "            if not phone_number.isdigit():\n",
        "                error_df = pd.concat([error_df, pd.DataFrame([row])], ignore_index=True)\n",
        "                df.drop(index, inplace=True)\n",
        "            else:\n",
        "                # Update the dataframe with the cleaned phone number\n",
        "                df.at[index, phone_column] = phone_number\n",
        "\n",
        "        print(\"Phone number validation complete. Invalid phone number records appended to error_df.\")\n",
        "        return df, error_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during phone number validation: {e}\")\n",
        "        return df, pd.DataFrame()  # Return empty error dataframe in case of error\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_H34efzBV3k"
      },
      "source": [
        "Function to check and truncate any fields over 52 bits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tD6oqOSBZNV"
      },
      "outputs": [],
      "source": [
        "# prompt: Create a function to check and truncate any fields over 52 bits.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def truncate_large_fields(df, columns_to_check=None):\n",
        "    \"\"\"\n",
        "    Checks and truncates any fields in a dataframe that exceed 52 bits in size.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe to process.\n",
        "        columns_to_check (list, optional): A list of column names to check.\n",
        "            If None, all numeric columns are checked.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The dataframe with truncated values for fields exceeding 52 bits.\n",
        "    \"\"\"\n",
        "    if columns_to_check is None:\n",
        "        columns_to_check = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "    for column in columns_to_check:\n",
        "        if pd.api.types.is_numeric_dtype(df[column]):\n",
        "            df[column] = df[column].apply(lambda x: min(x, (2**52) - 1) if x > (2**52) - 1 else x)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7wxU8XJCUE7"
      },
      "source": [
        "Function to remove non-utf characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEEcaDIxClOO"
      },
      "outputs": [],
      "source": [
        "# prompt: Create a function to remove non-utf characters from a dataset.\n",
        "\n",
        "def remove_non_utf_characters(df):\n",
        "    \"\"\"\n",
        "    Removes non-UTF-8 characters from all string columns in a Pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to process.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with non-UTF-8 characters removed from string columns.\n",
        "    \"\"\"\n",
        "    for column in df.select_dtypes(include=['object']):\n",
        "        df[column] = df[column].apply(lambda x: x.encode('utf-8', 'ignore').decode('utf-8') if isinstance(x, str) else x)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "py9AXaKGaV3n"
      },
      "outputs": [],
      "source": [
        "# prompt: Create a function to remove non-utf characters from specified columns in a dataset.\n",
        "\n",
        "def remove_non_utf_characters_from_columns(df, columns_to_check):\n",
        "    \"\"\"\n",
        "    Removes non-UTF-8 characters from specified columns in a Pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to process.\n",
        "        columns_to_check (list): A list of column names to check and remove non-UTF-8 characters from.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with non-UTF-8 characters removed from specified columns.\n",
        "    \"\"\"\n",
        "    for column in columns_to_check:\n",
        "        if column in df.columns:\n",
        "            df[column] = df[column].apply(lambda x: x.encode('utf-8', 'ignore').decode('utf-8') if isinstance(x, str) else x)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt075FBO3Z9J"
      },
      "source": [
        "Function to validate names in a datafaframe, converting characters with accents to regular characters. This code snippet removes records with invalid characters but should be duplicated and refined to remove the invalid characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsPtK-7CytdW"
      },
      "outputs": [],
      "source": [
        "# prompt: Create a function to validate names in a dataframe, detecting and removing any invalid characters, convert any alphabetical characters with accents to regular alphabetical characters and drop them from the original dataframe. Return the updated dataframe. Include error checking and logging.\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "def validate_and_remove_invalid_names(df, name_column):\n",
        "    \"\"\"\n",
        "    Validates names in a dataframe, detects and removes any invalid characters,\n",
        "    converts any alphabetical characters with accents to regular alphabetical\n",
        "    characters, and removes them from the original dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe containing names.\n",
        "        name_column (str): The name of the column containing names.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the updated dataframe with valid names and a\n",
        "            new dataframe with records containing invalid names.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Regular expression to match invalid characters in names\n",
        "        #invalid_char_regex = r\"[^a-zA-Z\\s]\" # must be updated to include \"-\"\n",
        "        invalid_char_regex = r\"[^a-zA-Z\\s\\-']\" # includes - like in Pitt-Browne and ' as in O'Connor\n",
        "\n",
        "        # Create a new dataframe to store records with invalid names\n",
        "        error_df = pd.DataFrame()\n",
        "\n",
        "        # Iterate through the dataframe and validate names\n",
        "        for index, row in df.iterrows():\n",
        "            name = row[name_column]\n",
        "            if re.search(invalid_char_regex, name):\n",
        "                # Append record to the error dataframe\n",
        "                error_df = pd.concat([error_df, pd.DataFrame([row])], ignore_index=True)\n",
        "                # Drop the record from the original dataframe\n",
        "                df.drop(index, inplace=True)\n",
        "            else:\n",
        "                # Remove accents from the name\n",
        "                name = ''.join((c for c in unicodedata.normalize('NFD', name) if unicodedata.category(c) != 'Mn'))\n",
        "                df.at[index, name_column] = name\n",
        "\n",
        "        print(\"Name validation complete. Invalid name records appended to error_df.\")\n",
        "        return df, error_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during name validation: {e}\")\n",
        "        return df, pd.DataFrame()  # Return empty error dataframe in case of error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV6yqwH22uEE"
      },
      "outputs": [],
      "source": [
        "# prompt: Create a function to validate names in a dataframe based on specified columns, detecting and removing any invalid characters, convert any alphabetical characters with accents to regular alphabetical characters and drop them from the original dataframe. Return the updated dataframe. Include error checking and logging.\n",
        "\n",
        "def validate_and_remove_invalid_names_in_columns(df, name_columns):\n",
        "    \"\"\"\n",
        "    Validates names in a dataframe based on specified columns, detecting and removing any invalid characters,\n",
        "    converts any alphabetical characters with accents to regular alphabetical characters, and drops them\n",
        "    from the original dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe containing names.\n",
        "        name_columns (list): A list of column names containing names to validate.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The updated dataframe with valid names.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        invalid_char_regex = r\"[^a-zA-Z\\s\\-']\"  # Includes '-', like in Pitt-Browne, and ' as in O'Connor\n",
        "\n",
        "        for name_column in name_columns:\n",
        "            if name_column in df.columns:\n",
        "                # Create a copy of the dataframe to avoid modifying the original\n",
        "                df_copy = df.copy()\n",
        "\n",
        "                # Iterate through the dataframe and validate names\n",
        "                for index, row in df_copy.iterrows():\n",
        "                    name = row[name_column]\n",
        "                    if name is not None and isinstance(name, str) and re.search(invalid_char_regex, name):\n",
        "                        # Drop the row if invalid characters are found\n",
        "                        df.drop(index, inplace=True)\n",
        "                    else:\n",
        "                        # Remove accents from the name\n",
        "                        if name is not None and isinstance(name, str):\n",
        "                            name = ''.join((c for c in unicodedata.normalize('NFD', name) if unicodedata.category(c) != 'Mn'))\n",
        "                            df.at[index, name_column] = name\n",
        "\n",
        "                print(f\"Name validation complete for column: {name_column}. Rows with invalid names removed.\")\n",
        "            else:\n",
        "                print(f\"Warning: Column '{name_column}' not found in the dataframe.\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during name validation: {e}\")\n",
        "        return df  # Return the original dataframe in case of error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srhky5f4n6x4"
      },
      "source": [
        "Function to go through each CSV chunk, run the validation functions and merge the chunks into the final CSV. **Use when chunks are created using chunksize in read_csv.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdo9HN9poPph"
      },
      "outputs": [],
      "source": [
        "# prompt: Create a function to get chunked csvs from a specified folder, runs the validation functions and merges the chunks into a specified final valid csv file and final error csv file. INclude error checking and logging.\n",
        "\n",
        "import csv\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import os\n",
        "import logging\n",
        "\n",
        "def process_chunked_csvs(input_folder, output_valid_csv, output_error_csv, email_column_name='email', phone_column_name='phone', name_column_name='name'):\n",
        "  \"\"\"\n",
        "  Processes chunked CSV files from a specified folder, runs validation functions,\n",
        "  and merges the results into final valid and error CSV files.\n",
        "\n",
        "  Args:\n",
        "      input_folder (str): The path to the folder containing chunked CSV files.\n",
        "      output_valid_csv (str): The path to the output CSV file for valid records.\n",
        "      output_error_csv (str): The path to the output CSV file for error records.\n",
        "      email_column_name (str): The name of the email column.\n",
        "      phone_column_name (str): The name of the phone column.\n",
        "      name_column_name (str): The name of the name column.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    # Setup logging\n",
        "    logging.basicConfig(filename='processing_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    valid_df = pd.DataFrame()\n",
        "    error_df = pd.DataFrame()\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "      if filename.endswith(\".csv\"):\n",
        "        file_path = os.path.join(input_folder, filename)\n",
        "        logging.info(f\"Processing file: {file_path}\")\n",
        "\n",
        "        try:\n",
        "          df = pd.read_csv(file_path, low_memory=False) #add the column names from the first chunk\n",
        "\n",
        "          # Run validation functions\n",
        "          df, chunk_error_df = validate_and_remove_invalid_emails(df, email_column_name)\n",
        "          df, chunk_duplicates_df = remove_duplicate_records(df, columns)\n",
        "          df, chunk_error_df_phone = validate_and_remove_invalid_phone_numbers(df, phone_column_name)\n",
        "          df, chunk_error_df_name = validate_and_remove_invalid_names(df, name_column_name)\n",
        "          df = truncate_large_fields(df)\n",
        "          df = remove_non_utf_characters(df)\n",
        "\n",
        "          # Concatenate error dataframes\n",
        "          chunk_error_df = pd.concat([chunk_error_df, chunk_error_df_phone, chunk_error_df_name], ignore_index=True)\n",
        "\n",
        "\n",
        "          valid_df = pd.concat([valid_df, df], ignore_index=True)\n",
        "          error_df = pd.concat([error_df, chunk_error_df], ignore_index=True)\n",
        "\n",
        "          logging.info(f\"File {file_path} processed successfully.\")\n",
        "\n",
        "        except Exception as e:\n",
        "          logging.error(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "    # Save final dataframes\n",
        "    valid_df.to_csv(output_valid_csv, index=False)\n",
        "    error_df.to_csv(output_error_csv, index=False)\n",
        "\n",
        "    logging.info(f\"Final valid data saved to {output_valid_csv}.\")\n",
        "    logging.info(f\"Final error data saved to {output_error_csv}.\")\n",
        "\n",
        "  except Exception as e:\n",
        "    logging.critical(f\"Critical error during processing: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "process_chunked_csvs('/content/chunks', 'final_valid_data.csv', 'final_error_data.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_2YKkzcqnPY"
      },
      "source": [
        "# *Final code with necessary functions*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6yo-iqBW3_f",
        "outputId": "f54d44b6-792b-4686-fd1f-8cd2226bb984"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-7d4ea2876337>:137: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, low_memory=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation complete. Invalid email records appended to error_df.\n",
            "Validation complete. Invalid email records appended to error_df.\n",
            "Validation complete. Invalid email records appended to error_df.\n",
            "Validation complete. Invalid email records appended to error_df.\n",
            "Duplicate removal complete. Duplicate records appended to duplicate_df.\n"
          ]
        }
      ],
      "source": [
        "# prompt: Create a function to get chunked csvs from a specified folder, runs the validation functions and merges the chunks into a specified final valid csv file and final error csv file. INclude error checking and logging.\n",
        "\n",
        "import csv\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import os\n",
        "import logging\n",
        "import datetime as dt\n",
        "\n",
        "\n",
        "# prompt: Create a function to remove duplicate records based on specified columns. Add the duplicates records to separate dataframe and drop them from the original. Include error checking and logging.\n",
        "\n",
        "def remove_duplicate_records(df, columns):\n",
        "    \"\"\"\n",
        "    Removes duplicate records based on specified columns.\n",
        "    Adds duplicate records to a separate dataframe and drops them from the original.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe to process.\n",
        "        columns (list): A list of column names to consider for duplicate detection.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the updated dataframe with unique records\n",
        "               and a new dataframe with duplicate records.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        duplicate_df = pd.DataFrame()\n",
        "        df_deduplicated = df.drop_duplicates(subset=columns, keep='first')\n",
        "        duplicate_rows = df[~df.index.isin(df_deduplicated.index)]\n",
        "\n",
        "        if not duplicate_rows.empty:\n",
        "            duplicate_df = pd.concat([duplicate_df, duplicate_rows], ignore_index=True)\n",
        "\n",
        "        print(f\"Duplicate removal complete. Duplicate records appended to duplicate_df.\")\n",
        "        return df_deduplicated, duplicate_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during duplicate removal: {e}\")\n",
        "        return df, pd.DataFrame()\n",
        "\n",
        "\n",
        "# prompt: Create a function to remove the time from a date in specified columns\n",
        "\n",
        "def remove_time_from_date(df, columns):\n",
        "  \"\"\"Removes the time component from date columns in a DataFrame.\n",
        "\n",
        "  Args:\n",
        "    df: The DataFrame containing the date columns.\n",
        "    columns: A list of column names to process.\n",
        "\n",
        "  Returns:\n",
        "    The DataFrame with the time component removed from the specified columns.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    for column in columns:\n",
        "      if column in df.columns:\n",
        "        # Convert to datetime if not already\n",
        "        df[column] = pd.to_datetime(df[column], errors='coerce')\n",
        "        # Remove the time component\n",
        "        df[column] = df[column].dt.date\n",
        "\n",
        "    return df\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# prompt: create a function to validate emails addresses in a dataframe, append the records with an invalid email address to a dataframe and drop them from the original dataframe. Return both the updated dataframe and the error dataframe. Include error checking and loggging.\n",
        "\n",
        "def validate_and_remove_invalid_emails(df, email_column):\n",
        "    \"\"\"\n",
        "    Validates email addresses in a dataframe, appends records with invalid email\n",
        "    addresses to a new dataframe, and removes them from the original dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe containing email addresses.\n",
        "        email_column (str): The name of the column containing email addresses.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the updated dataframe with valid email addresses\n",
        "            and a new dataframe with records containing invalid email addresses.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Regular expression for basic email validation\n",
        "        email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
        "\n",
        "        # Create a new dataframe to store records with invalid email addresses\n",
        "        error_df = pd.DataFrame()\n",
        "\n",
        "        # Iterate through the dataframe and validate email addresses\n",
        "        for index, row in df.iterrows():\n",
        "            email = row[email_column]\n",
        "            if not re.match(email_regex, email):\n",
        "                # Append record to the error dataframe\n",
        "                error_df = pd.concat([error_df, pd.DataFrame([row])], ignore_index=True)\n",
        "                # Drop the record from the original dataframe\n",
        "                df.drop(index, inplace=True)\n",
        "\n",
        "        print(\"Validation complete. Invalid email records appended to error_df.\")\n",
        "        return df, error_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during email validation: {e}\")\n",
        "        return df, pd.DataFrame()  # Return empty error dataframe in case of error\n",
        "\n",
        "\n",
        "# prompt: Create a function to get chunked csvs from a specified folder, runs the validation functions and merges the chunks into a specified final valid csv file and final error csv file. INclude error checking and logging.\n",
        "\n",
        "def process_chunked_csvs(input_folder, output_valid_csv, output_error_csv, output_duplicates_csv, email_column_name='mail_address', date_columns=['created_at']):\n",
        "  \"\"\"\n",
        "  Processes chunked CSV files from a specified folder, runs validation functions,\n",
        "  and merges the results into final valid and error CSV files.\n",
        "\n",
        "  Args:\n",
        "      input_folder (str): The path to the folder containing chunked CSV files.\n",
        "      output_valid_csv (str): The path to the output CSV file for valid records.\n",
        "      output_error_csv (str): The path to the output CSV file for error records.\n",
        "      output_duplicates_csv (str): The path to the output CSV file for duplicate records.\n",
        "      email_column_name (str): The name of the email column.\n",
        "      date_columns (list): A list of column names to consider for date validation.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    # Setup logging\n",
        "    logging.basicConfig(filename='processing_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    valid_df = pd.DataFrame()\n",
        "    error_df = pd.DataFrame()\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "      if filename.endswith(\".csv\"):\n",
        "        file_path = os.path.join(input_folder, filename)\n",
        "        logging.info(f\"Processing file: {file_path}\")\n",
        "\n",
        "        try:\n",
        "          df = pd.read_csv(file_path, low_memory=True)\n",
        "\n",
        "          # Run validation functions\n",
        "          df, chunk_error_df = validate_and_remove_invalid_emails(df, email_column_name)\n",
        "          df = remove_time_from_date(df, date_columns)\n",
        "          #df['gender'] = df['gender'].astype(int)\n",
        "          #df['birthday_on'] = df['birthday_on'].dt.date\n",
        "          # df = truncate_large_fields(df)\n",
        "          # df = remove_non_utf_characters(df)\n",
        "\n",
        "          valid_df = pd.concat([valid_df, df], ignore_index=True)\n",
        "\n",
        "          # Concatenate error dataframes\n",
        "          #chunk_error_df = pd.concat([chunk_error_df,chunk_dup_error_df], ignore_index=True)\n",
        "          error_df = pd.concat([error_df, chunk_error_df], ignore_index=True)\n",
        "\n",
        "          logging.info(f\"File {file_path} processed successfully.\")\n",
        "\n",
        "        except Exception as e:\n",
        "          logging.error(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "    # Remove duplicates from full dataframe\n",
        "    valid_df, duplicates_df = remove_duplicate_records(valid_df, ['mail_address']) #remove duplicates based on email\n",
        "    #error_df = pd.concat([error_df, chunk_dup_error_df], ignore_index=True)\n",
        "\n",
        "    # Save final dataframes\n",
        "    valid_df.to_csv(output_valid_csv, index=False)\n",
        "    error_df.to_csv(output_error_csv, index=False)\n",
        "    duplicates_df.to_csv(output_duplicates_csv, index=False)\n",
        "\n",
        "    logging.info(f\"Final valid data saved to {output_valid_csv}.\")\n",
        "    logging.info(f\"Final error data saved to {output_error_csv}.\")\n",
        "    logging.info(f\"Final duplicates data saved to {output_duplicates_csv}.\")\n",
        "\n",
        "  except Exception as e:\n",
        "    logging.critical(f\"Critical error during processing: {e}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "process_chunked_csvs('/content/chunks', 'final_valid_data.csv', 'final_error_data.csv', 'duplicates_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogTchYHSwt3X",
        "outputId": "e0b7439f-d405-489a-c8f2-81efb05590c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-8c5b2044843f>:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/final_valid_data.csv')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id    login_id              mail_address                          password  \\\n",
            "0   1    sugimoto   sugimoto@lifebear.co.jp  f0bac04aa1b45cf443d722d6f71c0250   \n",
            "1   2         kou  nakanishi@lifebear.co.jp  48207c322ee5bb156ffec9f08c960aaa   \n",
            "2   3      yusuke     yuozawa1208@gmail.com  048261a8024ce51d379eb53cc51aaf33   \n",
            "3   4  entyan1106        endo1106@gmail.com  cd77a9dac26260a104facda5665eb3ab   \n",
            "4   5      kuriki          kuriki@wavy4.com  a026597c294cc48cd20ae361f10cbab1   \n",
            "\n",
            "   created_at          salt birthday_on  gender  \n",
            "0  2012-01-13  yGwBKynnsctI  1984-11-09     0.0  \n",
            "1  2012-01-14  aha6EuRYCDvU  1986-11-13     0.0  \n",
            "2  2012-01-17  PVS59dPWk9BH  1984-12-08     0.0  \n",
            "3  2012-01-17  vLZI6TVCJowN  1987-11-06     0.0  \n",
            "4  2012-01-17  swFznWWk79fg  1986-10-21     0.0  \n"
          ]
        }
      ],
      "source": [
        "# prompt: Create code to read a csv and show sample of dataframe\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_file.csv' with the actual path to your CSV file\n",
        "df = pd.read_csv('/content/final_valid_data.csv')\n",
        "\n",
        "# Show a sample of the dataframe (e.g., the first 5 rows)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFnKqa7nAuAG",
        "outputId": "031a5bd9-6230-45e2-f244-fd7259ef2d31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-07609f5dc364>:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/lifebear.csv', sep=\";\", low_memory=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               id         login_id                    mail_address  \\\n",
            "136           138        maaam1120             ammma1120@gmail.com   \n",
            "221           223         exuernok            2br02b1215@gmail.com   \n",
            "227           229     takayuki0930          takayuki0930@gmail.com   \n",
            "332           334           hiromi       hiromi.sono.111@gmail.com   \n",
            "621           623          UZUMAME       uzumame.uzumame@gmail.com   \n",
            "...           ...              ...                             ...   \n",
            "3679894  11593957          0enaka0           dara0o0arad@gmail.com   \n",
            "3679926  11594071         ns109097             ns109097@icloud.com   \n",
            "3679964  11594208      takayamasae        as_coco_0520@yahoo.co.jp   \n",
            "3680042  11594511          hyx0630             hyx0630@ezweb.ne.jp   \n",
            "3680353  11595638  relapishoptest9  relapishoptest9@lifebear.co.jp   \n",
            "\n",
            "                                 password           created_at          salt  \\\n",
            "136      f2dea97eab78a50d6cc615c2f172c890  2012-05-28 11:58:21  48dpW9JT7u1w   \n",
            "221      214a676dc79cddbaa42b25a91b0409f0  2012-06-02 04:45:38  FtBEDlKNBr6Z   \n",
            "227      157c0fc86ae997f44a92ade83f56d3e0  2012-06-04 10:48:38  QVsw2y4RgI4o   \n",
            "332      72a6b960035a9d5438b4d09614da6bdb  2012-07-27 23:30:32  t5m1mYhRmbO3   \n",
            "621      2177d847a7e8cd37ed321637a86ce66c  2012-07-29 22:24:48  q90atNIh5iV9   \n",
            "...                                   ...                  ...           ...   \n",
            "3679894  7f1dec1c78f4f63ac4849911a7856ade  2019-02-21 00:26:34  yWbfrArUapNA   \n",
            "3679926  22650ec4dd2951d15277adc82180512d  2019-02-21 00:53:41  cHQlQyZa2acX   \n",
            "3679964  70dd2e962bec6c420c3b9753cd701800  2019-02-21 01:40:27  0W7RlMpljZHT   \n",
            "3680042  66a02d47b178164266e4b8a6593ec81e  2019-02-21 07:10:00  SELwhAAPwOgS   \n",
            "3680353  faa17f36e63271e1c0f50d42cb57509d  2019-02-21 14:42:20  pJsuJpLNsXm0   \n",
            "\n",
            "        birthday_on  gender  \n",
            "136      1983-11-20     0.0  \n",
            "221      1986-12-15     0.0  \n",
            "227      1982-09-30     0.0  \n",
            "332      1976-01-11     1.0  \n",
            "621      1996-07-05     1.0  \n",
            "...             ...     ...  \n",
            "3679894         NaN     NaN  \n",
            "3679926         NaN     1.0  \n",
            "3679964         NaN     1.0  \n",
            "3680042         NaN     NaN  \n",
            "3680353         NaN     NaN  \n",
            "\n",
            "[17870 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "# prompt: Generate code to show all duplicate mail_address from lifebear.csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Replace 'lifebear.csv' with the actual path to your CSV file\n",
        "df = pd.read_csv('/content/lifebear.csv', sep=\";\", low_memory=True)\n",
        "\n",
        "# Check if 'mail_address' exists in the DataFrame\n",
        "if 'mail_address' in df.columns:\n",
        "    # Find duplicate mail_address entries\n",
        "    duplicate_emails = df[df.duplicated(subset=['mail_address'], keep=False)]  # keep=False shows all duplicates\n",
        "    # Print the duplicate email addresses\n",
        "    print(duplicate_emails)\n",
        "else:\n",
        "    print(\"The 'mail_address' column does not exist in the DataFrame.\")\n",
        "\n",
        "duplicate_emails.to_csv('duplicate_emails.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
