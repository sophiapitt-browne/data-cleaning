{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# All Imports"
      ],
      "metadata": {
        "id": "5zVLPrjIjqx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import os\n",
        "import logging\n",
        "import datetime as dt"
      ],
      "metadata": {
        "id": "ii7iDeWXjpcm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Declare All Functions"
      ],
      "metadata": {
        "id": "5gjZIrI1kSj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup logging\n",
        "logging.basicConfig(filename='processing_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# prompt: Create a function to remove duplicate records based on specified columns. Add the duplicates records to separate dataframe and drop them from the original. Include error checking and logging.\n",
        "\n",
        "def remove_duplicate_records(df, columns):\n",
        "    \"\"\"\n",
        "    Removes duplicate records based on specified columns.\n",
        "    Adds duplicate records to a separate dataframe and drops them from the original.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe to process.\n",
        "        columns (list): A list of column names to consider for duplicate detection.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the updated dataframe with unique records\n",
        "               and a new dataframe with duplicate records.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Processing dataframe to remove and store duplicate records.\")\n",
        "        logging.info(f\"Processing dataframe to remove and store duplicate records.\")\n",
        "        duplicate_df = pd.DataFrame()\n",
        "        df_deduplicated = df.drop_duplicates(subset=columns, keep='first')\n",
        "        duplicate_rows = df[~df.index.isin(df_deduplicated.index)]\n",
        "\n",
        "        if not duplicate_rows.empty:\n",
        "            duplicate_df = pd.concat([duplicate_df, duplicate_rows], ignore_index=True)\n",
        "\n",
        "        print(f\"Duplicate removal complete. Duplicate records appended to duplicate_df.\")\n",
        "        logging.info(f\"Duplicate removal complete. Duplicate records appended to duplicate_df.\")\n",
        "        return df_deduplicated, duplicate_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during duplicate removal: {e}\")\n",
        "        logging.error(f\"Error occurred during duplicate removal: {e}\")\n",
        "        return df, pd.DataFrame()\n",
        "\n",
        "# prompt: Create a function to process a specified CSV file and then run the function to remove duplicates and convert the valid and duplicates dataframes to csv files.\n",
        "\n",
        "def process_duplicates_csv(file_path, output_valid_csv, output_duplicates_csv, columns, sep=','):\n",
        "    \"\"\"\n",
        "    Processes a single CSV file, removes duplicates, and outputs valid and duplicate dataframes to CSV files.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, sep=sep, low_memory=True)\n",
        "        print(f\"Processing CSV file: {file_path} for duplicates.\")\n",
        "        logging.info(f\"Processing CSV file: {file_path} for duplicates.\")\n",
        "\n",
        "        # Remove duplicates based on email\n",
        "        df, duplicates_df = remove_duplicate_records(df, columns)\n",
        "\n",
        "        # Save valid and duplicate dataframes to CSV files\n",
        "        df.to_csv(output_valid_csv, index=False)\n",
        "        duplicates_df.to_csv(output_duplicates_csv, index=False)\n",
        "\n",
        "        print(f\"Processed file: {file_path}. Valid data saved to {output_valid_csv}, duplicates to {output_duplicates_csv}.\")\n",
        "        logging.info(f\"Processed file: {file_path}. Valid data saved to {output_valid_csv}, duplicates to {output_duplicates_csv}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        logging.error(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "\n",
        "# prompt: Create a function to split a large csv into chunks in a specified folder or path using the chunksize parameter in read_csv\n",
        "\n",
        "def split_csv_into_chunks(file_path, chunksize, output_directory, sep=','):\n",
        "  \"\"\"Splits a large CSV file into smaller chunks using the chunksize parameter.\n",
        "\n",
        "  Args:\n",
        "    file_path: The path to the large CSV file.\n",
        "    chunksize: The number of rows per chunk.\n",
        "    output_directory: The directory where the chunks should be saved.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    print(f\"Processing CSV file: {file_path} to split into chunks.\")\n",
        "    logging.info(f\"Processing CSV file: {file_path} to split into chunks.\")\n",
        "\n",
        "    if not os.path.exists(output_directory):\n",
        "      os.makedirs(output_directory)\n",
        "\n",
        "    for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunksize, sep=sep)):\n",
        "      output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "      chunk.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"File '{file_path}' split into {i+1} chunks in '{output_directory}'.\")\n",
        "    logging(f\"File '{file_path}' split into {i+1} chunks in '{output_directory}'.\")\n",
        "\n",
        "  except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    logging.error(f\"Error: {e}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An unexpected error occurred when splitting CSV into chunks: {e}\")\n",
        "    logging.error(f\"An unexpected error occurred when splitting CSV into chunks: {e}\")\n",
        "\n",
        "# prompt: Create a function to get chunked CSVs from a specified folder, runs the validation functions and outputs the cleaned chunks in specified folders. Include error checking and logging.\n",
        "\n",
        "def process_chunked_csvs_output_folders(input_folder, output_valid_folder, output_error_folder, email_column_name='mail_address', date_columns=['created_at']):\n",
        "  \"\"\"\n",
        "  Processes chunked CSV files from a specified folder, runs validation functions,\n",
        "  and outputs the cleaned chunks in specified folders. Includes error checking and logging.\n",
        "\n",
        "  Args:\n",
        "      input_folder (str): The path to the folder containing chunked CSV files.\n",
        "      output_valid_folder (str): The path to the folder to output cleaned chunks.\n",
        "      output_error_folder (str): The path to the folder to output chunks with errors.\n",
        "      email_column_name (str): The name of the email column.\n",
        "      date_columns (list): A list of column names to consider for date validation.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    # Setup logging\n",
        "    #logging.basicConfig(filename='processing_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "    print(f\"Processing chunks from: {input_folder} for data cleaning and output to folders.\")\n",
        "    logging.info(f\"Processing chunks from: {input_folder} for data cleaning and output to folders.\")\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "      if filename.endswith(\".csv\"):\n",
        "        file_path = os.path.join(input_folder, filename)\n",
        "        print(f\"Processing file: {file_path}\")\n",
        "        logging.info(f\"Processing file: {file_path}\")\n",
        "\n",
        "        try:\n",
        "          df = pd.read_csv(file_path, low_memory=True)\n",
        "\n",
        "          # Run validation functions\n",
        "          #df['gender'] = df['gender'].replace({0.0: 'M', 1.0: 'F'})\n",
        "          df, chunk_error_df = validate_and_remove_invalid_emails(df, email_column_name)\n",
        "          df = remove_time_from_date(df, date_columns)\n",
        "\n",
        "          # Output cleaned chunk\n",
        "          if not os.path.exists(output_valid_folder):\n",
        "            os.makedirs(output_valid_folder)\n",
        "\n",
        "          output_valid_file = os.path.join(output_valid_folder, f\"valid_{filename}\")\n",
        "          df.to_csv(output_valid_file, index=False)\n",
        "          print(f\"Final valid data saved to {output_valid_file}.\")\n",
        "          logging.info(f\"Final valid data saved to {output_valid_file}.\")\n",
        "\n",
        "          # Output chunk with errors\n",
        "          if not os.path.exists(output_error_folder):\n",
        "            os.makedirs(output_error_folder)\n",
        "\n",
        "          output_error_file = os.path.join(output_error_folder, f\"error_{filename}\")\n",
        "          chunk_error_df.to_csv(output_error_file, index=False)\n",
        "          print(f\"Final error data saved to {output_error_file}.\")\n",
        "          logging.info(f\"Final error data saved to {output_error_file}.\")\n",
        "\n",
        "          print(f\"File from {file_path} processed successfully.\")\n",
        "          logging.info(f\"File {file_path} processed successfully.\")\n",
        "\n",
        "        except Exception as e:\n",
        "          print(f\"Error processing file {file_path}: {e}\")\n",
        "          logging.error(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Critical error during processing: {e}\")\n",
        "    logging.critical(f\"Critical error during processing: {e}\")\n",
        "\n",
        "# prompt: Create a function to remove the time from a date in specified columns\n",
        "\n",
        "def remove_time_from_date(df, columns):\n",
        "  \"\"\"Removes the time component from date columns in a DataFrame.\n",
        "\n",
        "  Args:\n",
        "    df: The DataFrame containing the date columns.\n",
        "    columns: A list of column names to process.\n",
        "\n",
        "  Returns:\n",
        "    The DataFrame with the time component removed from the specified columns.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    print(f\"Removing time from specified date columns: {columns}\")\n",
        "    logging.info(f\"Removing time from specified date columns: {columns}\")\n",
        "    for column in columns:\n",
        "      if column in df.columns:\n",
        "        # Convert to datetime if not already\n",
        "        df[column] = pd.to_datetime(df[column], errors='coerce')\n",
        "        # Remove the time component\n",
        "        df[column] = df[column].dt.date\n",
        "    print(\"Date cleaning complete. Time removed from date columns.\")\n",
        "    logging.info(\"Date cleaning complete. Time removed from date columns.\")\n",
        "    return df\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred during removing time from date function: {e}\")\n",
        "    logging.error(f\"An error occurred during removing time from date function: {e}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# prompt: create a function to validate emails addresses in a dataframe, append the records with an invalid email address to a dataframe and drop them from the original dataframe. Return both the updated dataframe and the error dataframe. Include error checking and loggging.\n",
        "\n",
        "def validate_and_remove_invalid_emails(df, email_column):\n",
        "    \"\"\"\n",
        "    Validates email addresses in a dataframe, appends records with invalid email\n",
        "    addresses to a new dataframe, and removes them from the original dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe containing email addresses.\n",
        "        email_column (str): The name of the column containing email addresses.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the updated dataframe with valid email addresses\n",
        "            and a new dataframe with records containing invalid email addresses.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Removing invalid emails from {email_column}.\")\n",
        "        logging.info(f\"Removing invalid emails from {email_column}.\")\n",
        "\n",
        "        # Regular expression for basic email validation\n",
        "        email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
        "\n",
        "        # Create a new dataframe to store records with invalid email addresses\n",
        "        error_df = pd.DataFrame()\n",
        "\n",
        "        # Iterate through the dataframe and validate email addresses\n",
        "        for index, row in df.iterrows():\n",
        "            email = row[email_column]\n",
        "            if not re.match(email_regex, email):\n",
        "                # Append record to the error dataframe\n",
        "                error_df = pd.concat([error_df, pd.DataFrame([row])], ignore_index=True)\n",
        "                # Drop the record from the original dataframe\n",
        "                df.drop(index, inplace=True)\n",
        "\n",
        "        print(\"Validation complete. Invalid email records appended to error_df.\")\n",
        "        logging.info(\"Validation complete. Invalid email records appended to error_df.\")\n",
        "        return df, error_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during email validation: {e}\")\n",
        "        logging.error(f\"Error occurred during email validation: {e}\")\n",
        "        return df, pd.DataFrame()  # Return empty error dataframe in case of error\n",
        "\n",
        "\n",
        "# prompt: Create a function to get chunked csvs from a specified folder, runs the validation functions and merges the chunks into a specified final valid csv file and final error csv file. INclude error checking and logging.\n",
        "\n",
        "def process_chunked_csvs(input_folder, output_valid_csv, output_error_csv, email_column_name='mail_address', date_columns=['created_at']):\n",
        "  \"\"\"\n",
        "  Processes chunked CSV files from a specified folder, runs validation functions,\n",
        "  and merges the results into final valid and error CSV files.\n",
        "\n",
        "  Args:\n",
        "      input_folder (str): The path to the folder containing chunked CSV files.\n",
        "      output_valid_csv (str): The path to the output CSV file for valid records.\n",
        "      output_error_csv (str): The path to the output CSV file for error records.\n",
        "      output_duplicates_csv (str): The path to the output CSV file for duplicate records.\n",
        "      email_column_name (str): The name of the email column.\n",
        "      date_columns (list): A list of column names to consider for date validation.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    # Setup logging\n",
        "    #logging.basicConfig(filename='processing_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    valid_df = pd.DataFrame()\n",
        "    error_df = pd.DataFrame()\n",
        "\n",
        "    print(f\"Processing chunks for data cleaning & combining from {input_folder} to single cleaned file: {output_valid_csv}\")\n",
        "    logging.info(f\"Processing chunks for data cleaning & combining from {input_folder} to single cleaned file: {output_valid_csv}\")\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "      if filename.endswith(\".csv\"):\n",
        "        file_path = os.path.join(input_folder, filename)\n",
        "        print(f\"Processing chunk: {file_path}\")\n",
        "        logging.info(f\"Processing chunk: {file_path}\")\n",
        "\n",
        "        try:\n",
        "          df = pd.read_csv(file_path, low_memory=True)\n",
        "\n",
        "          # Run validation functions\n",
        "          #df['gender'] = df['gender'].replace({0.0: 'M', 1.0: 'F'})\n",
        "          df, chunk_error_df = validate_and_remove_invalid_emails(df, email_column_name)\n",
        "          df = remove_time_from_date(df, date_columns)\n",
        "          #df['gender'] = df['gender'].astype(int)\n",
        "          #df['birthday_on'] = df['birthday_on'].dt.date\n",
        "          # df = truncate_large_fields(df)\n",
        "          # df = remove_non_utf_characters(df)\n",
        "\n",
        "          valid_df = pd.concat([valid_df, df], ignore_index=True)\n",
        "\n",
        "          # Concatenate error dataframes\n",
        "          #chunk_error_df = pd.concat([chunk_error_df,chunk_dup_error_df], ignore_index=True)\n",
        "          error_df = pd.concat([error_df, chunk_error_df], ignore_index=True)\n",
        "\n",
        "          print(f\"File {file_path} processed successfully.\")\n",
        "          logging.info(f\"File {file_path} processed successfully.\")\n",
        "\n",
        "        except Exception as e:\n",
        "          print(f\"Error processing file {file_path}: {e}\")\n",
        "          logging.error(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "    # Remove duplicates from full dataframe\n",
        "    #valid_df, duplicates_df = remove_duplicate_records(valid_df, ['mail_address']) #remove duplicates based on email\n",
        "    #error_df = pd.concat([error_df, chunk_dup_error_df], ignore_index=True)\n",
        "\n",
        "    # Save final dataframes\n",
        "    valid_df.to_csv(output_valid_csv, index=False)\n",
        "    error_df.to_csv(output_error_csv, index=False)\n",
        "    #duplicates_df.to_csv(output_duplicates_csv, index=False)\n",
        "\n",
        "    print(f\"Final cleaned data saved to {output_valid_csv}.\")\n",
        "    logging.info(f\"Final cleaned data saved to {output_valid_csv}.\")\n",
        "    print(f\"Final garbage data saved to {output_error_csv}.\")\n",
        "    logging.info(f\"Final garbage data saved to {output_error_csv}.\")\n",
        "    #logging.info(f\"Final duplicates data saved to {output_duplicates_csv}.\")\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Critical error during processing of chunks: {e}\")\n",
        "    logging.critical(f\"Critical error during processing of chunks: {e}\")\n",
        "\n",
        "# prompt: Create a function that will combine csv chunks from a specified folder into one csv file\n",
        "\n",
        "def combine_csv_chunks(input_folder, output_file):\n",
        "  \"\"\"\n",
        "  Combines multiple CSV chunks from a folder into a single CSV file.\n",
        "\n",
        "  Args:\n",
        "    input_folder: The path to the folder containing CSV chunks.\n",
        "    output_file: The path to the output CSV file.\n",
        "  \"\"\"\n",
        "\n",
        "  combined_df = pd.DataFrame()\n",
        "  print(f\"Combining CSV chunks from {input_folder}\")\n",
        "  logging.info(f\"Combining CSV chunks from {input_folder}\")\n",
        "\n",
        "  for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(\".csv\"):\n",
        "      file_path = os.path.join(input_folder, filename)\n",
        "      try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
        "      except Exception as e:\n",
        "        print(f\"Error reading file {file_path}: {e}\")\n",
        "        logging.error(f\"Error reading file {file_path}: {e}\")\n",
        "\n",
        "  combined_df.to_csv(output_file, index=False)\n",
        "  print(f\"Combined CSV chunks saved to {output_file}\")\n",
        "  logging.info(f\"Combined CSV chunks saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "4lQOFeO0kWnl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optional Step: Function to remove invalid rows from a CSV**"
      ],
      "metadata": {
        "id": "Ck7XaHgC8jDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Create a function that removes invalid rows from a CSV. Detect the number of columns based on the header and the specified delimiter.\n",
        "# Rows where the number of columns don't match the expected number of columns and may cause problems when trying to use the read_csv function.\n",
        "# Create a new CSV file with the valid rows.\n",
        "\n",
        "def remove_invalid_rows(input_file, output_file, delimiter=','):\n",
        "  \"\"\"Removes invalid rows from a CSV file.\n",
        "\n",
        "  Args:\n",
        "    input_file: Path to the input CSV file.\n",
        "    output_file: Path to the output CSV file.\n",
        "    delimiter: Delimiter used in the CSV file.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
        "       open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
        "\n",
        "    reader = csv.reader(infile, delimiter=delimiter)\n",
        "    writer = csv.writer(outfile, delimiter=delimiter)\n",
        "\n",
        "    header = next(reader)  # Read the header row\n",
        "    expected_num_columns = len(header)\n",
        "    writer.writerow(header)  # Write the header to the output file\n",
        "\n",
        "    for row in reader:\n",
        "      if len(row) == expected_num_columns:\n",
        "        writer.writerow(row)\n",
        "\n",
        "# prompt: Create a function that removes invalid rows from a CSV. Detect the number of columns based on the header and the specified delimiter.\n",
        "# Rows where the number of columns don't match the expected number of columns and may cause problems when trying to use the read_csv function.\n",
        "\n",
        "def remove_invalid_rows_from_csv(csv_file_path, delimiter=','):\n",
        "  \"\"\"Removes rows from a CSV file that have an invalid number of columns.\n",
        "\n",
        "  Args:\n",
        "    csv_file_path: The path to the CSV file.\n",
        "    delimiter: The delimiter used in the CSV file.\n",
        "\n",
        "  Returns:\n",
        "    A list of valid rows.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(csv_file_path, 'r', encoding='utf-8') as file:\n",
        "    reader = csv.reader(file, delimiter=delimiter)\n",
        "    header = next(reader)  # Get the header row\n",
        "    expected_num_columns = len(header)\n",
        "    valid_rows = [header]  # Start with the header\n",
        "\n",
        "    for row in reader:\n",
        "      if len(row) == expected_num_columns:\n",
        "        valid_rows.append(row)\n",
        "      else:\n",
        "        print(f\"Warning: Skipping row with invalid number of columns: {row}\")\n",
        "\n",
        "  return valid_rows\n",
        "\n",
        "# Example usage:\n",
        "# valid_rows = remove_invalid_rows_from_csv('my_file.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "bvyt3sIN8pcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Function to run: check for duplicates, then use the valid CSV to create the chunks for further processing.**"
      ],
      "metadata": {
        "id": "NnpCWYqnYXKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for Duplicates in the Original CSV:\n",
        "process_duplicates_csv('/content/lifebear.csv', 'valid_data.csv', 'duplicate_data.csv', sep=';', columns=['login_id', 'mail_address'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr4tzL_0WTxL",
        "outputId": "bee98962-b5f8-45e1-b299-0364e6157825"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-9471dffa8647>:38: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, sep=sep, low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate removal complete. Duplicate records appended to duplicate_df.\n",
            "Processed file: /content/lifebear.csv. Valid data saved to valid_data.csv, duplicates to duplicate_data.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ** Step 2: Function to split chunks based on the chunksize**"
      ],
      "metadata": {
        "id": "eNpxcFzhuAY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the large CSV into chunks for further processing:\n",
        "# Replace 'your_large_file.csv' with the actual path to your file\n",
        "# Replace 'output_chunks_folder' with the desired output directory\n",
        "#split_csv_into_chunks('/content/lifebear.csv', 1000000, '/content/chunks', sep=';')\n",
        "split_csv_into_chunks('/content/valid_data.csv', 1000000, '/content/chunks', sep=',')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHk3n7fvvrUU",
        "outputId": "28cfbbf7-c769-4d7e-9b26-85662881bd59"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-9471dffa8647>:66: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunksize, sep=sep)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File '/content/valid_data.csv' split into 4 chunks in '/content/chunks'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Step 3: Run data cleaning functions and export chunks to specified output folders*"
      ],
      "metadata": {
        "id": "ru6h0nJOSnsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run data cleaning functions and export chunks to specified output folders:\n",
        "process_chunked_csvs_output_folders('/content/chunks', '/content/cleaned_chunks', '/content/error_chunks')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63KGWBgBSoFm",
        "outputId": "e4db3a47-b35e-4470-bffb-378f418d7d86"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation complete. Invalid email records appended to error_df.\n",
            "Validation complete. Invalid email records appended to error_df.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-9471dffa8647>:323: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation complete. Invalid email records appended to error_df.\n",
            "Validation complete. Invalid email records appended to error_df.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Step 3 (Alternate): Run data cleaning functions and combine chunks to specified CSVs*"
      ],
      "metadata": {
        "id": "P_2YKkzcqnPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get chunked csvs from a specified folder, runs the validation functions and merges the chunks into a specified final valid csv file and final error csv file. INclude error checking and logging.\n",
        "# Run data cleaning functions and combine chunks to specified CSVs\n",
        "process_chunked_csvs('/content/chunks', 'final_valid_data.csv', 'final_error_data.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6yo-iqBW3_f",
        "outputId": "fb9077e6-047a-4ae4-a957-4fe918b0bebf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation complete. Invalid email records appended to error_df.\n",
            "Validation complete. Invalid email records appended to error_df.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-9471dffa8647>:258: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation complete. Invalid email records appended to error_df.\n",
            "Validation complete. Invalid email records appended to error_df.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 (Optional): Function to Combine Chunks into a Single CSV"
      ],
      "metadata": {
        "id": "-WaIuBaHfR3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine chunks into a single CSV:\n",
        "# Replace 'your_chunks_folder' and 'combined_file.csv' with your actual paths\n",
        "combine_csv_chunks('/content/cleaned_chunks', 'combined_cleaned_data.csv')\n"
      ],
      "metadata": {
        "id": "luRHf5DifQgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f36b5b-b07d-4720-ed33-ee84e2e900e2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combining CSV chunks from /content/cleaned_chunks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-bf292930f25a>:328: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined CSV chunks saved to combined_cleaned_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Create code to read a csv and show sample of dataframe\n",
        "\n",
        "# Replace 'your_file.csv' with the actual path to your CSV file\n",
        "df = pd.read_csv('/content/final_valid_data.csv')\n",
        "\n",
        "# Show a sample of the dataframe (e.g., the first 5 rows)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogTchYHSwt3X",
        "outputId": "e0b7439f-d405-489a-c8f2-81efb05590c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8c5b2044843f>:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/final_valid_data.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id    login_id              mail_address                          password  \\\n",
            "0   1    sugimoto   sugimoto@lifebear.co.jp  f0bac04aa1b45cf443d722d6f71c0250   \n",
            "1   2         kou  nakanishi@lifebear.co.jp  48207c322ee5bb156ffec9f08c960aaa   \n",
            "2   3      yusuke     yuozawa1208@gmail.com  048261a8024ce51d379eb53cc51aaf33   \n",
            "3   4  entyan1106        endo1106@gmail.com  cd77a9dac26260a104facda5665eb3ab   \n",
            "4   5      kuriki          kuriki@wavy4.com  a026597c294cc48cd20ae361f10cbab1   \n",
            "\n",
            "   created_at          salt birthday_on  gender  \n",
            "0  2012-01-13  yGwBKynnsctI  1984-11-09     0.0  \n",
            "1  2012-01-14  aha6EuRYCDvU  1986-11-13     0.0  \n",
            "2  2012-01-17  PVS59dPWk9BH  1984-12-08     0.0  \n",
            "3  2012-01-17  vLZI6TVCJowN  1987-11-06     0.0  \n",
            "4  2012-01-17  swFznWWk79fg  1986-10-21     0.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate code to show all duplicate mail_address from lifebear.csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Replace 'lifebear.csv' with the actual path to your CSV file\n",
        "df = pd.read_csv('/content/lifebear.csv', sep=\";\", low_memory=True)\n",
        "\n",
        "# Check if 'mail_address' exists in the DataFrame\n",
        "if 'mail_address' in df.columns:\n",
        "    # Find duplicate mail_address entries\n",
        "    duplicate_emails = df[df.duplicated(subset=['mail_address'], keep=False)]  # keep=False shows all duplicates\n",
        "    # Print the duplicate email addresses\n",
        "    print(duplicate_emails)\n",
        "else:\n",
        "    print(\"The 'mail_address' column does not exist in the DataFrame.\")\n",
        "\n",
        "duplicate_emails.to_csv('duplicate_emails.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFnKqa7nAuAG",
        "outputId": "031a5bd9-6230-45e2-f244-fd7259ef2d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-07609f5dc364>:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/lifebear.csv', sep=\";\", low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               id         login_id                    mail_address  \\\n",
            "136           138        maaam1120             ammma1120@gmail.com   \n",
            "221           223         exuernok            2br02b1215@gmail.com   \n",
            "227           229     takayuki0930          takayuki0930@gmail.com   \n",
            "332           334           hiromi       hiromi.sono.111@gmail.com   \n",
            "621           623          UZUMAME       uzumame.uzumame@gmail.com   \n",
            "...           ...              ...                             ...   \n",
            "3679894  11593957          0enaka0           dara0o0arad@gmail.com   \n",
            "3679926  11594071         ns109097             ns109097@icloud.com   \n",
            "3679964  11594208      takayamasae        as_coco_0520@yahoo.co.jp   \n",
            "3680042  11594511          hyx0630             hyx0630@ezweb.ne.jp   \n",
            "3680353  11595638  relapishoptest9  relapishoptest9@lifebear.co.jp   \n",
            "\n",
            "                                 password           created_at          salt  \\\n",
            "136      f2dea97eab78a50d6cc615c2f172c890  2012-05-28 11:58:21  48dpW9JT7u1w   \n",
            "221      214a676dc79cddbaa42b25a91b0409f0  2012-06-02 04:45:38  FtBEDlKNBr6Z   \n",
            "227      157c0fc86ae997f44a92ade83f56d3e0  2012-06-04 10:48:38  QVsw2y4RgI4o   \n",
            "332      72a6b960035a9d5438b4d09614da6bdb  2012-07-27 23:30:32  t5m1mYhRmbO3   \n",
            "621      2177d847a7e8cd37ed321637a86ce66c  2012-07-29 22:24:48  q90atNIh5iV9   \n",
            "...                                   ...                  ...           ...   \n",
            "3679894  7f1dec1c78f4f63ac4849911a7856ade  2019-02-21 00:26:34  yWbfrArUapNA   \n",
            "3679926  22650ec4dd2951d15277adc82180512d  2019-02-21 00:53:41  cHQlQyZa2acX   \n",
            "3679964  70dd2e962bec6c420c3b9753cd701800  2019-02-21 01:40:27  0W7RlMpljZHT   \n",
            "3680042  66a02d47b178164266e4b8a6593ec81e  2019-02-21 07:10:00  SELwhAAPwOgS   \n",
            "3680353  faa17f36e63271e1c0f50d42cb57509d  2019-02-21 14:42:20  pJsuJpLNsXm0   \n",
            "\n",
            "        birthday_on  gender  \n",
            "136      1983-11-20     0.0  \n",
            "221      1986-12-15     0.0  \n",
            "227      1982-09-30     0.0  \n",
            "332      1976-01-11     1.0  \n",
            "621      1996-07-05     1.0  \n",
            "...             ...     ...  \n",
            "3679894         NaN     NaN  \n",
            "3679926         NaN     1.0  \n",
            "3679964         NaN     1.0  \n",
            "3680042         NaN     NaN  \n",
            "3680353         NaN     NaN  \n",
            "\n",
            "[17870 rows x 8 columns]\n"
          ]
        }
      ]
    }
  ]
}